{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8929038,"sourceType":"datasetVersion","datasetId":5371198}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# A simple example to get started using GUIDE\nimport os\nimport pandas as pd\n\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:35:46.360986Z","iopub.execute_input":"2024-07-31T15:35:46.361895Z","iopub.status.idle":"2024-07-31T15:35:49.151214Z","shell.execute_reply.started":"2024-07-31T15:35:46.361768Z","shell.execute_reply":"2024-07-31T15:35:49.149961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Validation\n\n* Explore the first 10.000 rows of the dataset to determine data preparation strategy","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/microsoft-security-incident-prediction/GUIDE_Train.csv', nrows=10000)  # read a few rows to start","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:48:33.455162Z","iopub.execute_input":"2024-07-31T15:48:33.455588Z","iopub.status.idle":"2024-07-31T15:48:33.53364Z","shell.execute_reply.started":"2024-07-31T15:48:33.455556Z","shell.execute_reply":"2024-07-31T15:48:33.532441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['Category'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:35:49.272959Z","iopub.execute_input":"2024-07-31T15:35:49.273351Z","iopub.status.idle":"2024-07-31T15:35:49.289915Z","shell.execute_reply.started":"2024-07-31T15:35:49.273317Z","shell.execute_reply":"2024-07-31T15:35:49.288755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['EvidenceRole'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:36:16.119745Z","iopub.execute_input":"2024-07-31T15:36:16.120182Z","iopub.status.idle":"2024-07-31T15:36:16.129371Z","shell.execute_reply.started":"2024-07-31T15:36:16.120147Z","shell.execute_reply":"2024-07-31T15:36:16.128042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['CountryCode'].unique()","metadata":{"execution":{"iopub.status.busy":"2024-07-31T15:40:09.141204Z","iopub.execute_input":"2024-07-31T15:40:09.141609Z","iopub.status.idle":"2024-07-31T15:40:09.152154Z","shell.execute_reply.started":"2024-07-31T15:40:09.141578Z","shell.execute_reply":"2024-07-31T15:40:09.15082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.iloc[:,:15].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.iloc[:, [0, 9] + list(range(15, 30))].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.iloc[:, [0, 9] + list(range(30, 45))].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Count summary of Incident Grade classes\ntrain_data['IncidentGrade'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Percentage count summary of Incident Grade classes\ntrain_data['IncidentGrade'].value_counts() * 100 / train_data['IncidentGrade'].shape[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# def prepare_data():\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"le_cat_columns = ['Category', 'EntityType', 'EvidenceRole', 'SuspicionLevel', 'LastVerdict',\n                  'ResourceType', 'Roles', 'AntispamDirection', 'ThreatFamily','CountryCode',\n                  'OSFamily', 'OSVersion','State', 'City', 'RegistryValueName', 'RegistryValueData', \n                  'ResourceIdName', 'RegistryKey', 'OAuthApplicationId', 'ApplicationId', 'ApplicationName']\n\nnumerical_columns = ['DeviceId', 'Sha256', 'IpAddress', 'Url', 'AccountSid', 'AccountUpn', 'AccountObjectId',\n                     'AccountName', 'DeviceName', 'NetworkMessageId', 'EmailClusterId', 'FileName', 'FolderPath']\n\nle_cat_columns += numerical_columns\n\nnumerical_columns = []\n\nohe_cat_columns = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[le_cat_columns].nunique().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[numerical_columns].nunique().sort_values(ascending=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Inspect columns with number of unique values less than 10\nfor col in train_data:\n    if train_data[col].nunique() < 10:\n        print(col, train_data[col].unique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration","metadata":{}},{"cell_type":"code","source":"def preprocess_data(df, le_cat_columns):\n    \"\"\"\n        This function preprocesses the dataset\n    \"\"\"\n    \n    # Converts columns with fewer than 20 unique values to ohe categorical columns\n    df['Timestamp'] = pd.to_datetime(df['Timestamp'])\n    \n    # Converts numerical to label encoded categorical columns\n    for le_col in le_cat_columns:\n        df[le_col] = df[le_col].astype('object')\n\n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = preprocess_data(train_data, le_cat_columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data[le_cat_columns].nunique())\nprint(train_data[ohe_cat_columns].nunique())\nprint(train_data[numerical_columns].nunique())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### ANOVA F-Statistic\n\n**Goal**: To assess the statistical significance of a feature in predicting the target variable.\n\nAfter encoding the categorical features as numerical values, ANOVA (Analysis of Variance) is used to measure the significance of each feature.\n\n**Method:**\n\nANOVA compares the means of different groups and determines if the differences between those means are statistically significant.\nThe larger the F-statistic, the more significant the feature is as a predictor.\n\n**Findings:**\n\n* **Country Code**, **State** and **City**  are the most significant predictors.\n* **Resource Type**, **RegistryValueName**, **RegistryValueData**, **Roles** do not appear to have strong significance as predictors.\n\nTo assess the statistical significance of a feature on the target variable.\n\nAfter the categorical feature is encoded as numerical values, ANOVA is used to measure the significance of the feature.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_selection import f_classif\n\ncat_columns = ohe_cat_columns + le_cat_columns\n\n# stats_data = pd.DataFrame()\n\nfor cat in cat_columns:\n    # One-Hot Encode the categorical data\n    onehot_encoder = OneHotEncoder(sparse_output=False)  # Adjust for the FutureWarning\n    X_encoded = onehot_encoder.fit_transform(train_data[[cat]])  # Use double brackets to pass a 2D array\n    \n    # ANOVA F-Statistic\n    label_encoder = LabelEncoder()\n    y = label_encoder.fit_transform(train_data['IncidentGrade'])  # Assuming IncidentGrade is categorical\n    f_statistic, p_value = f_classif(X_encoded, y)\n    \n    print(\"*\" * 20)\n    print(f\"Feature: {cat}\")\n    print(f\"ANOVA F-Statistic: {f_statistic}\")\n    print(f\"p-Value: {p_value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert 'Timestamp' column to datetime\ntrain_data['Timestamp'] = pd.to_datetime(train_data['Timestamp'])\n\ntrain_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"def process_data():\n    train_data = pd.read_csv('/kaggle/input/microsoft-security-incident-prediction/GUIDE_Train.csv') \n    test_data = pd.read_csv('/kaggle/input/microsoft-security-incident-prediction/GUIDE_Test.csv')\n    \n    print(train_data.shape)\n    \n    # Drop rows with missing target variable 'IncidentGrade'\n    train_data.dropna(subset=['IncidentGrade'], inplace=True)\n    \n    train_data = preprocess_data(train_data, le_cat_columns)\n    test_data = preprocess_data(test_data, le_cat_columns)\n    \n    group_columns = ohe_cat_columns + numerical_columns + le_cat_columns\n    \n    # Drop duplicates based on the specified columns\n    train_data = train_data.drop_duplicates(subset=group_columns)\n    \n    # Drop usage column as it is not present in train dataset\n    test_data.drop(['Usage'], axis=1, inplace=True)\n    \n    print(train_data.shape)\n    print(test_data.shape)\n    \n    #  One hot encoding\n    ohe = OneHotEncoder(handle_unknown='ignore')\n    ohe.fit(train_data[ohe_cat_columns])\n\n    train_data_ohe = csr_matrix(ohe.transform(train_data[ohe_cat_columns]))\n    test_data_ohe = csr_matrix(ohe.transform(test_data[ohe_cat_columns]))\n\n    # Fill NaNs for numerical columns\n    train_data_numerical = csr_matrix(train_data[numerical_columns].fillna(-1).values)\n    test_data_numerical = csr_matrix(test_data[numerical_columns].fillna(-1).values)\n    \n    # Feature label encoding\n    feature_le = LabelEncoder()\n    \n    train_data_le = pd.DataFrame()\n    test_data_le = pd.DataFrame()\n    \n    # Fit and transform the feature variables\n    for le_col in le_cat_columns:\n        # we want to stack train and test for label encoding of some cat variables\n        feature_le.fit(pd.concat([train_data[le_col], test_data[le_col]]))\n        train_data_le[le_col] = feature_le.transform(train_data[le_col])\n        test_data_le[le_col] = feature_le.transform(test_data[le_col])\n    \n    train_data_le = csr_matrix(train_data_le)\n    test_data_le = csr_matrix(test_data_le)\n    \n    X_train = hstack([train_data_ohe, train_data_le ,train_data_numerical])\n    X_test = hstack([test_data_ohe, test_data_le, test_data_numerical])\n\n    # Target label encoding\n    target_le = LabelEncoder()\n    \n    # Fit and transform the target variable\n    target_le.fit(train_data['IncidentGrade'])\n    y_train = target_le.transform(train_data['IncidentGrade'])\n    y_test = target_le.transform(test_data['IncidentGrade'])\n    \n    # Print out the label classes of the target variable\n    \"\"\"\n        0: 'BenignPositive'\n        1: 'FalsePositive'\n        2: 'TruePositive'\n    \"\"\"\n    print(f\"Target Classes: {target_le.classes_}\")\n        \n    return X_train, y_train, X_test, y_test\n    \n    \n# get the data\nX_train, y_train, X_test, y_test = process_data()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling and Evaluation","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, classification_report, roc_auc_score, roc_curve, auc\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndef predict(model, X_test, y_test):\n    # Generate predictions\n    y_pred = model.predict(X_test)\n    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n    \n    # Print accuracy\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n    \n    # Print classification report\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred))\n    \n    # Print confusion matrix\n    print(\"\\nConfusion Matrix:\")\n    \n    cm = confusion_matrix(y_test, y_pred)\n    \n    cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, \n                                        display_labels = ['BenignPositive', 'FalsePositive', 'TruePositive'])\n\n    cm_display.plot()\n    plt.show()\n\n    return y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Classifier","metadata":{}},{"cell_type":"code","source":"def train_random_forest_classifier(X_train, y_train):\n    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\n    \n    model.fit(X_train, y_train)\n    \n    # Feature importance\n    importances = model.feature_importances_\n    \n    feature_columns = np.array(ohe_cat_columns + le_cat_columns + numerical_columns)\n    \n    # Plot feature importance\n    indices = np.argsort(importances)[::-1]\n    \n    plt.figure(figsize=(12, 6))\n    plt.title(\"Feature Importances (Random Forest Classifier)\")\n    plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n    plt.xticks(range(X_train.shape[1]), feature_columns[indices], rotation=90)\n    plt.xlim([-1, X_train.shape[1]])\n    plt.show()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train a random forest classifier model\nrfc_model = train_random_forest_classifier(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\ny_pred = predict(rfc_model, X_test, y_test)\n\n# evaluate test performance\naccuracy = accuracy_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred, average='macro')\nprecision = precision_score(y_test, y_pred, average='macro')\n\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint('Accuracy: {}'.format(accuracy))\nprint('Macro-Precision: {}'.format(precision))\nprint('Macro-Recall: {}'.format(recall))\nprint('Macro-F1 Score: {}'.format(f1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost Classifier","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n\ndef train_xgboost_classifier(X_train, y_train):\n    model = XGBClassifier(n_estimators=100, max_depth=5, random_state=0, use_label_encoder=False, eval_metric='mlogloss')\n    \n    model.fit(X_train, y_train)\n    \n    # Feature importance\n    importances = model.feature_importances_\n    \n    feature_columns = np.array(ohe_cat_columns + le_cat_columns + numerical_columns)\n    \n    # Plot feature importance\n    indices = np.argsort(importances)[::-1]\n    \n    plt.figure(figsize=(12, 6))\n    plt.title(\"Feature Importances (XGBoost Classifier)\")\n    plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n    plt.xticks(range(X_train.shape[1]), feature_columns[indices], rotation=90)\n    plt.xlim([-1, X_train.shape[1]])\n    plt.show()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train a random forest classifier model\nxgb_model = train_xgboost_classifier(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\ny_pred = predict(xgb_model, X_test, y_test)\n\n# evaluate test performance\naccuracy = accuracy_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred, average='macro')\nprecision = precision_score(y_test, y_pred, average='macro')\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint('Accuracy: {}'.format(accuracy))\nprint('Macro-Precision: {}'.format(precision))\nprint('Macro-Recall: {}'.format(recall))\nprint('Macro-F1 Score: {}'.format(f1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"from catboost import CatBoostClassifier\n\ndef train_catboost_classifier(X_train, y_train):\n    model = CatBoostClassifier(iterations=100, depth=5, random_seed=0, verbose=0)\n    \n    model.fit(X_train, y_train)\n    \n    # Feature importance\n    importances = model.get_feature_importance()\n    \n    feature_columns = np.array(ohe_cat_columns + le_cat_columns + numerical_columns)\n    \n    # Plot feature importance\n    indices = np.argsort(importances)[::-1]\n    \n    plt.figure(figsize=(12, 6))\n    plt.title(\"Feature Importances (CatBoost Classifier)\")\n    plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\")\n    plt.xticks(range(X_train.shape[1]), feature_columns[indices], rotation=90)\n    plt.xlim([-1, X_train.shape[1]])\n    plt.show()\n\n    return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train a random forest classifier model\ncat_model = train_catboost_classifier(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# make predictions\ny_pred = predict(cat_model, X_test, y_test)\n\n# evaluate test performance\naccuracy = accuracy_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred, average='macro')\nprecision = precision_score(y_test, y_pred, average='macro')\nf1 = f1_score(y_test, y_pred, average='macro')\n\nprint('Accuracy: {}'.format(accuracy))\nprint('Macro-Precision: {}'.format(precision))\nprint('Macro-Recall: {}'.format(recall))\nprint('Macro-F1 Score: {}'.format(f1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}